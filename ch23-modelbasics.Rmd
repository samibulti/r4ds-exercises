---
title: "Chapter 23: Model basics"
output: html_notebook
---

```{r}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

# 23.2 A simple model

## 23.2.1 Exercises

1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
sim1a_mod <- lm(y ~ x, data = sim1a)
sim1a %>%
  ggplot(aes(x, y)) +
    geom_point() +
    geom_abline(intercept = coef(sim1a_mod)[1], slope = coef(sim1a_mod)[2])
```

**When there are some strong outliers the line of best fit can be dragged up to a point where it may not seem to represent the central tendency in the relationship between the variables. In the example I've saved at this stage, there are three very high y values and as a result of them, the line is at or above almost all of the other data points - only 4 points are clearly above the line, with the remaining 26 at or below it.**

2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}
```

Use ```optim()``` to fit this model to the simulated data above and compare it to the linear model.

```{r}
make_prediction <- function(a, data) {
  a[1] + data$x * a[2]
}

best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par
```

```{r}
ggplot(sim1a, aes(x, y)) + 
  geom_point() + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

**With the absolute distance instead of root-mean-squared distance, the line representing the model isn't dragged up as far by the outliers and appears to do a better job of passing through the middle of the path where most observations are distributed, with a roughly even number of observations above and below the model's line.**

3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this?

```
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```

**In this case two parameters, `a[1]` and `a[3]` both contribute to the intercept. The numerical method would presumably optimise one of these parameters while holding the other fixed, but there is no way to differentiate between those estimates and any other combination of those two parameters that sum to the same value.**

# 23.3 Visualising models

## 23.3.3 Exercises

1. Instead of using `lm()` to fit a straight line, you can use `loess()` to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on `sim1` using `loess()` instead of `lm()`. How does the result compare to `geom_smooth()`?

```{r}
sim1_mod_loess <- loess(y ~ x, data = sim1)
grid_loess <- sim1 %>%
  data_grid(x) %>%
  add_predictions(sim1_mod_loess)
ggplot(sim1, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_line(data = grid_loess, aes(y = pred), colour = "red", size = 1)
```

```{r}
sim1 %>%
  ggplot(aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(colour = "red", size = 1)
```

**It produces the same line (note that `geom_smooth()` uses loess as its default method).**

2. `add_predictions()` is paired with `gather_predictions()` and `spread_predictions()`. How do these three functions differ?

```{r}
?add_predictions
```

**`add_predictions()` takes only a single model and adds its predictions as a new column.  `spread_predictions()` and `gather_predictions()` both take multiple models - `spread_predictions()` will put the predictions of each model in its pwn column, whereas `gather_predictions()` will replicate the rows and add columns labelled `.model` and `.pred` so it can add the predictions from each model into the same columns.**

3. What does `geom_ref_line()` do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?

```{r}
?geom_ref_line
```

**It's from `modelr` and it adds a reference line, i.e., a horizontal or vertical line, to a `ggplot2` plot. I can see two ways you might use reference lines. First, because the distance of residuals from zero is the indicator of how much the predictions are off the mark, you could have a horizontal reference line at zero. Second, if you have an idea of acceptable/unacceptable thresholds for residuals then you could add horizontal lines at those points, e.g., to create a band around zero.**

4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?

```{r}
sim1 %>%
  add_residuals(sim1_mod_loess) %>%
  ggplot(aes(x = abs(resid))) +
    geom_freqpoly()
```

```{r}
sim1 %>%
  add_residuals(sim1_mod_loess) %>%
  ggplot(aes(x = x, y = resid)) +
    geom_point()
```

**The frequency polygon of absolute residuals gives a way of focussing on any patterns in the overall magnitude of distances between predicted and observed values. If the model is accounting for the major patterns in the data then absolute residual values should be low and closr to zero. If there are high frequencies of larger residuals then the model is away from the observations in many places. On the other hand, the plot of raw residuals allows us to look for any patterns in the distribution of residuals across the range of scores in the distribution. In most cases the ideal (and in terms of statistical procedures, the assumption) is that residuals are evenly distributed across the range (and normally distributed at all points along the range). The random-looking spread of residuals above fits that expectation. But if a model is showing unusually high or low residuals in parts of the distribution that may be of interest and warrant more exploration.**

# 23.4 Formulas and model families

## 23.4.5 Exercises

1. What happens if you repeat the analysis of `sim2` using a model without an intercept. What happens to the model equation? What happens to the predictions?

```{r}
# Model with intercept
model_matrix(sim2, y ~ x)
# Model without intercept
model_matrix(sim2, y ~ x -1)
```

**When the intercept is removed, an extra `xa` variable is added in its place.**

```{r}
mod2_noint <- lm(y ~ x - 1, data = sim2)

grid_noint <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2_noint)

grid_noint
```

**The predictions are unchanged from the model with intercept.**

```{r}
mod2_int <- lm(y ~ x, data = sim2)

mod2_int
mod2_noint
```

**The `xa` variable has taken on the same value as the intercept had, and the other coefficients are now equal to their value in the original model plus the intercept. That makes sense when you review the model matrices above - the intercept was always added to predictions in all groups.**

2. Use `model_matrix(`) to explore the equations generated for the models I fit to` sim3` and `sim4`. Why is `*` a good shorthand for interaction?

```{r}
model_matrix(sim3, y ~ x1 + x2)
model_matrix(sim3, y ~ x1 * x2)
model_matrix(sim4, y ~ x1 + x2)
model_matrix(sim4, y ~ x1 * x2)
```

**In each row of the model matrix, the coefficient for an interaction (e.g., `x1:x2b`) is equal to the product of the corresponding individual coefficients (e.g., `x1 = 1`, `x2b = 0`, `x1:x2b = 0`).**

3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.)

```
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

**`y = a_0 + (a_1 * x1) + (a_2b * x2b) + (a_2c * x2c) + (a_2d + x_2d)`**

**`y = a_0 + (a_1 * x1) + (a_2b * x2b) + (a_2c * x2c) + (a_2d + x_2d) + (a_12b * x1 * x2b) + (a_12c * x1 * x2c) + (a_12d * x1 * x2d)`**

4. For `sim4`, which of `mod1` and `mod2` is better? I think `mod2` does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim?

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

sim4 <- sim4 %>%
  gather_residuals(mod1, mod2)

ggplot(sim4, aes(x1, resid, colour = x2)) +
  geom_point() +
  facet_grid(model ~ x2)
```

**That's a start but it's definitely subtle. I might come back to do more exploration later.**
