---
title: "Chapter 23: Model basics"
output: html_notebook
---

```{r}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

# 23.2 A simple model

## 23.2.1 Exercises

1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
sim1a_mod <- lm(y ~ x, data = sim1a)
sim1a %>%
  ggplot(aes(x, y)) +
    geom_point() +
    geom_abline(intercept = coef(sim1a_mod)[1], slope = coef(sim1a_mod)[2])
```

**When there are some strong outliers the line of best fit can be dragged up to a point where it may not seem to represent the central tendency in the relationship between the variables. In the example I've saved at this stage, there are three very high y values and as a result of them, the line is at or above almost all of the other data points - only 4 points are clearly above the line, with the remaining 26 at or below it.**

2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}
```

Use ```optim()``` to fit this model to the simulated data above and compare it to the linear model.

```{r}
make_prediction <- function(a, data) {
  a[1] + data$x * a[2]
}

best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par
```

```{r}
ggplot(sim1a, aes(x, y)) + 
  geom_point() + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

**With the absolute distance instead of root-mean-squared distance, the line representing the model isn't dragged up as far by the outliers and appears to do a better job of passing through the middle of the path where most observations are distributed, with a roughly even number of observations above and below the model's line.**

3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this?

```
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```

**In this case two parameters, `a[1]` and `a[3]` both contribute to the intercept. The numerical method would presumably optimise one of these parameters while holding the other fixed, but there is no way to differentiate between those estimates and any other combination of those two parameters that sum to the same value.**
